{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b0d37628-d11d-43ca-bd70-44567aebbf41"
    }
   },
   "source": [
    "# 評判分析で文章のポジネガを判別しよう\n",
    "\n",
    "機械学習を用いた評判分析における記念碑的論文( http://www.cs.cornell.edu/home/llee/papers/sentiment.pdf )と同様のセットアップで分析を行い、論文の精度を上回れるかチャレンジしてみましょう！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b14e9b2d-e81d-4c46-aaa5-a21ead865efb"
    }
   },
   "source": [
    "## 0.前準備\n",
    "\n",
    "python versionの確認します。<br>\n",
    "jupyter notebookではシェルコマンドの文頭に\"!\"をつけるとそのシェルコマンドをnotebook上で実行することができます。<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.6.0\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "カレントディレクトリの確認とデータディレクトリの確認をします。<br>\n",
    "osモジュールを使うことでOS依存の機能を使えるようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['評判分析入門_normal.ipynb', 'README.md', '.ipynb_checkpoints', '評判分析入門_advanced.ipynb']\n"
     ]
    }
   ],
   "source": [
    "print( os.listdir(os.path.normpath(\"./\")) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['README', 'mix20_rand700_tokens.zip', 'tokens']\n"
     ]
    }
   ],
   "source": [
    "print( os.listdir(os.path.normpath(\"../dataset/\")) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.dataの読み込みとモジュールのインポート"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyenvなどを用いているとpandasなどがimportできない場合があります。<br>\n",
    "その可能性の１つとしてlocale（国毎に異なる単位）の設定不足があり得るので、ここではそれを明示的に操作します。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your default locale is None\n",
      "Your locale is set as ja_JP.UTF-8\n"
     ]
    }
   ],
   "source": [
    "def set_locale():\n",
    "    default = os.environ.get('LC_ALL')\n",
    "    print( \"Your default locale is\", default )\n",
    "    if default is None:\n",
    "        os.environ.setdefault('LC_ALL', 'ja_JP.UTF-8')\n",
    "        print( \"Your locale is set as ja_JP.UTF-8\" )\n",
    "\n",
    "set_locale()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回使うデータファイルのパスをpythonのリストとして取得します。<br>\n",
    "globはパス名を見つけたりparseしたりするモジュールです( http://docs.python.jp/3/library/glob.html )。<br>\n",
    "今回扱うデータは https://www.cs.cornell.edu/people/pabo/movie-review-data/ より取得しています。<br>\n",
    "データ構造は下記のようになっています。<br>\n",
    "- data\n",
    "    - README\n",
    "    - tokens\n",
    "        - neg\n",
    "            - file1.txt\n",
    "            - file2.txt\n",
    "            - ...\n",
    "        - pos\n",
    "            - file1.txt\n",
    "            - file2.txt\n",
    "            - ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "neg_files = glob.glob( os.path.normpath(\"../dataset/tokens/neg/*\") )\n",
    "pos_files = glob.glob( os.path.normpath(\"../dataset/tokens/pos/*\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "取得したファイルパスの確認。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../dataset/tokens/neg/cv424_tok-29318.txt', '../dataset/tokens/neg/cv631_tok-20288.txt']\n",
      "['../dataset/tokens/pos/cv439_tok-13632.txt', '../dataset/tokens/pos/cv180_tok-20034.txt']\n"
     ]
    }
   ],
   "source": [
    "print(neg_files[0:2])\n",
    "print(pos_files[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データ読み込みのテストをします。\n",
    "\n",
    "実際に文章を１つ読み込んでみて正しく読み込めているかを確認します。<br>\n",
    "本データは１つのファイルに１文で映画のレビュー文章が記載されています。<br>\n",
    "テキストの読み込みはエンコーディングの問題などでエラーが生じやすいので、慣れるまでは根気強くdebugしましょう。<br>\n",
    "\n",
    "無事に読み込めたら、具体的に１つファイルの中身を読み込んで内容を確認してみましょう。<br>\n",
    "sys はファイルサイズ取得などのシステム上の操作を行うモジュールです( http://docs.python.jp/3/library/sys.html )。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def text_reader(file_path):\n",
    "    python_version = sys.version_info.major\n",
    "    \n",
    "    if python_version >= 3:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                print(line)\n",
    "    else:\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "literally blink , and you'll miss _soldier_'s lone flash of wit : a fleeting glimpse of a computer screen listing futuristic supersoldier todd's ( kurt russell ) numerous war commendations , among them the \" plissken patch \" --referring , of course , to russell's character in john carpenter's _escape_from . . . _ movies . the rest of this sci-fi actioner is brainless junk though writer david webb peoples and director paul anderson serve up an interesting basic premise . in the future , a select few males are chosen at birth to be trained their whole life as soldiers , nothing more ; veteran soldier todd is among the best , if not _the_ best . but when a new genetically engineered brand of soldier is developed , todd and his ilk are rendered obsolete . >from here , the story takes a most uninspired turn . presumed dead , todd is dumped onto a trash dump planet , where he meets up with a peaceful community of people who look and act like extras from _the_postman_ . for reasons that are never clear , the new soldiers attack this trash planet , and the film becomes an outer space _rambo_ , boldly declaring , \" i'm gonna kill 'em all , sir ! \" thus ensues much machine gun action and laughable \" emotional \" content where todd finds the humanity within his tough exterior . it plays even worse than it sounds . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_reader(neg_files[11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回使うモジュールの情報をまとめておきます。<br>\n",
    "詳細な中身などに関してはご自身で調べてみてください。<br>\n",
    "- matplotlib : グラフなどを描写する<br>\n",
    "http://matplotlib.org/\n",
    "- pandas : dataframeでデータを扱い、集計や統計量算出などがすぐ実行できる<br>\n",
    "http://pandas.pydata.org/\n",
    "- collections : pythonで扱えるデータの型を提供する<br>\n",
    "http://docs.python.jp/3/library/collections.html\n",
    "- numpy : 行列などの数学的オブジェクトを扱う<br>\n",
    "http://www.numpy.org/\n",
    "- sklearn.feature_extraction.DictVectorizer : 辞書データをベクトルの形に変換する<br>\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html\n",
    "- sklearnのモデル : SVM, NB, RF<br>\n",
    "http://scikit-learn.org/stable/tutorial/basic/tutorial.html\n",
    "- sklearn.grid_search : パラメタの最適な組み合わせ見つける<br>\n",
    "http://scikit-learn.org/stable/modules/grid_search.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/takahi-i/.pyenv/versions/3.6.0/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/takahi-i/.pyenv/versions/3.6.0/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib # not used in this notebook\n",
    "import pandas as pd # not used in this notebook\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "from sklearn import svm, naive_bayes\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn import grid_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.特徴ベクトルの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unigramを作成する関数を定義します。<br>\n",
    "スペース区切りで単語を抽出し、その数をカウントする単純な関数となります。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_counter(string):\n",
    "    words = string.strip().split()\n",
    "    count_dict = collections.Counter(words)\n",
    "    return dict(count_dict)\n",
    "\n",
    "def get_unigram(file_path):\n",
    "    result = []\n",
    "    python_version = sys.version_info.major\n",
    "    \n",
    "    if python_version >= 3:\n",
    "        for file in file_path:\n",
    "            with open(file, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    count_dict = word_counter(line)\n",
    "                    result.append(count_dict)\n",
    "    else:\n",
    "        for file in file_path:\n",
    "            with open(file, 'r') as f:\n",
    "                for line in f:\n",
    "                    count_dict = word_counter(line)\n",
    "                    result.append(count_dict)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "関数の挙動を確認してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I': 2,\n",
       " 'YK.': 1,\n",
       " 'am': 1,\n",
       " 'analysis': 1,\n",
       " 'data': 1,\n",
       " 'love': 1,\n",
       " 'python.': 1,\n",
       " 'using': 1}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counter(\"I am YK. I love data analysis using python.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この関数を用いて、negative と positive 両方で unigram を作成します。<br>\n",
    "得られた2つのリストを合わせてモデルのインプット（説明変数）とします。\n",
    "リストの結合は \"+\" で実施できます。 ex.) [1] + [2] = [1,2]<br>\n",
    "negative と positive は各700文ずつありますが、そのうちいくつを使うかをここで指定します。初期設定では全てのデータを使うことになっていますが、後の過程で memory 不足になるようでしたらこの数を減らしてください。<br>\n",
    "\n",
    "また、 jupyter notebook では %% をつけることで magic commands ( https://ipython.org/ipython-doc/3/interactive/magics.html ) という便利なコマンドを実行できます。ここでは処理にかかる時間をセルに表示するコマンドを使用しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 305 ms, sys: 135 ms, total: 441 ms\n",
      "Wall time: 616 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "DATA_NUM = 700\n",
    "\n",
    "unigrams_data = get_unigram(neg_files[:DATA_NUM]) + get_unigram(pos_files[:DATA_NUM])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得られたunigram_dataを確認してみます。単語の出現数がカウントされていることが確認できます。<br>\n",
    "合わせてそのデータサイズも確認してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'summer': 5, 'catch': 5, '(': 3, '2001': 1, ')': 3, '1': 1, '1/2': 1, 'stars': 1, 'out': 2, 'of': 14, '4': 1, '.': 48, 'starring': 1, 'freddie': 2, 'prinze': 3, 'jr': 2, ',': 30, 'jessical': 1, 'biel': 3, 'matthew': 2, 'lillard': 1, 'fred': 1, 'ward': 1, 'jason': 1, 'gedrick': 1, 'brittany': 1, 'murphy': 1, 'bruce': 2, 'davison': 2, 'and': 12, 'brian': 1, 'dennehy': 1, 'screenplay': 2, 'by': 8, 'kevin': 2, 'falls': 5, 'john': 2, 'gatins': 2, 'story': 4, 'directed': 1, 'mike': 1, 'tollin': 1, 'rated': 1, 'pg-13': 1, 'approx': 1, '110': 1, 'minutes': 1, 'is': 12, 'a': 14, 'minor': 1, 'league': 3, 'effort': 1, 'nine': 1, 'innings': 1, 'banality': 1, 'with': 2, 'lineup': 1, 'stock': 1, 'situations': 2, 'stereotypical': 1, 'characters': 1, 'this': 5, 'feature': 1, 'like': 1, 'double': 1, 'header': 1, 'two': 1, 'sets': 1, 'clich駸': 1, 'for': 5, 'the': 22, 'price': 1, 'one': 2, 'not': 1, 'only': 3, 'do': 1, 'we': 2, 'get': 1, 'usual': 1, 'tired': 1, 'sports': 1, 'chestnuts': 1, 'but': 3, 'banal': 1, 'rich': 2, 'girl-poor': 1, 'boy': 1, 'love': 2, 'tossed': 1, 'in': 7, 'good': 1, 'measure': 1, 'an': 1, 'original': 1, 'moment': 2, 'loser': 1, 'as': 8, 'rare': 1, 'chicago': 1, 'cubs': 1, 'world': 1, 'series': 1, 'appearance': 1, 'based': 1, 'on': 4, 'merely': 2, 'lobs': 1, 'its': 1, 'plotline': 1, 'at': 8, 'audience': 1, 'that': 1, 'needed': 1, 'sent': 1, 'down': 1, 'seasoning': 1, 'more': 1, 'coaching': 1, 'centers': 1, 'around': 1, 'ryan': 4, 'dunne': 1, 'cape': 3, 'cod': 2, 'youth': 1, 'chosen': 1, 'to': 5, 'participate': 1, 'prestigious': 1, 'baseball': 3, 'supposedly': 1, 'showcase': 1, 'best': 1, 'young': 1, 'amateur': 1, 'college': 1, 'players': 1, 'country': 1, 'blue-collar': 1, 'kind': 1, 'guy': 1, 'he': 6, 'works': 1, 'his': 3, 'dad': 1, 'taking': 1, 'care': 1, 'lawns': 1, \"cod's\": 1, 'famous': 1, 'also': 2, 'are': 1, 'informed': 1, 'early': 1, 'own': 1, 'worst': 1, 'enemy': 1, 'has': 2, 'potential': 1, 'talent': 1, 'always': 2, 'seems': 1, 'self-destruct': 1, 'crucial': 1, 'so': 1, 'tries': 1, 'remained': 1, 'focused': 1, 'then': 2, 'meets': 1, 'tenley': 2, 'parrish': 1, 'jessica': 1, 'daughter': 1, 'blue': 1, 'bloods': 1, 'whose': 1, 'lawn': 1, 'manicures': 1, 'name': 1, 'hack': 1, 'screenwriter': 1, 'could': 2, 'invent': 1, 'unlike': 1, 'her': 2, 'snobbish': 1, 'counterparts': 1, 'if': 1, 'you': 2, \"can't\": 1, 'figure': 1, 'where': 1, 'all': 1, 'nonsense': 1, 'leads': 1, 'need': 1, 'remedial': 1, 'course': 1, 'film': 2, 'viewing': 1, 'pretty': 1, 'look': 1, 'performance': 1, 'mainly': 1, 'consists': 1, 'facial': 1, 'expressions': 1, ':': 6, 'puppy': 1, 'dog': 1, 'heartbreak': 1, 'frustration': 1, 'self-loathing': 1, 'or': 2, 'determination': 1, 'cries': 1, 'lot': 1, 'while': 1, 'acts': 1, 'smarmy': 1, 'class-conscious': 1, 'father': 1, 'beacon': 1, \"lillard's\": 1, 'fun-loving': 1, 'billy': 1, 'brubaker': 1, \"team's\": 1, 'catcher': 1, 'borrows': 1, 'stylings': 1, 'from': 1, 'other': 2, 'movies': 1, 'such': 1, 'bull': 1, 'durham': 1, 'natural': 1, 'no': 2, 'curve': 1, 'balls': 1, 'here': 1, 'sliders': 1, 'every': 1, 'pitch': 1, 'predictable': 1, 'blind': 1, 'umpire': 1, 'call': 1, 'movie': 3, 'strictly': 1, 'rookie': 1, 'moviemaking': 1, 'it': 1, 'much': 1, 'chance': 1, 'making': 1, 'hall': 1, 'fame': 1, 'dodgers': 1, 'have': 1, 'moving': 1, 'back': 1, 'brooklyn': 1, 'bob': 3, 'bloom': 3, 'critic': 1, 'journal': 1, 'courier': 1, 'lafayette': 1, 'can': 3, 'be': 3, 'reached': 1, 'e-mail': 1, '<a': 3, 'href=': 3, '\"': 6, 'mailto': 2, 'bloom@journal-courier': 1, 'com': 2, '>bloom@journal-courier': 1, 'com</a>': 1, 'bobbloom@iquest': 1, 'net': 2, '>bobbloom@iquest': 1, '</a>': 1, 'reviews': 2, 'found': 2, 'www': 1, 'jconline': 1, 'clicking': 1, 'golafayette': 1, \"bloom's\": 1, 'web': 1, 'internet': 1, 'database': 1, 'http': 1, '//www': 2, 'imdb': 2, 'com/m/reviews_by': 2, '?': 2, '+': 2, '>http': 1, 'bloom</a>': 1}\n",
      "data size : 0.011264 [MB]\n"
     ]
    }
   ],
   "source": [
    "print( unigrams_data[0] )\n",
    "print( \"data size :\", sys.getsizeof(unigrams_data) / 1000000, \"[MB]\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上で得られたデータは unigram という 1400 の要素を持つリストであり、各要素は key と value からなる辞書となっています。<br>\n",
    "これを扱いやすい行列の形にします。<br>\n",
    "ここでは各行が１つのレビューテキストに対応するようにして、各列が単語、要素がその単語の出現数というデータを作成します。<br>\n",
    "scikit-learn で実装されている DictVectorizer という関数を使うことでそれが簡単に実行できます。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 497 ms, sys: 22.9 ms, total: 520 ms\n",
      "Wall time: 527 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vec = DictVectorizer()\n",
    "feature_vectors_csr = vec.fit_transform( unigrams_data )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作成したデータを確認してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1400x44219 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 496525 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_vectors_csr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "行列の全成分（行成分×列成分）は 60,000,000 要素くらいありますが、このうちのほとんどは 0 で 0 以外の値が入っているのは500,000程度です。<br>\n",
    "この CSR(Compressed Sparse Row) matrix というのはこのような疎行列をの 0 でない成分だけを保持する賢いものになっています。<br>\n",
    "\n",
    "一方で 0 の成分を陽に保って普通の行列としてデータを保持することも可能です。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data dimension : (1400, 44219)\n",
      "[0. 0. 6. ... 0. 0. 0.]\n",
      "data size : 495.252912 [MB]\n"
     ]
    }
   ],
   "source": [
    "feature_vectors = vec.fit_transform( unigrams_data ).toarray()\n",
    "print( \"data dimension :\", feature_vectors.shape )\n",
    "print( feature_vectors[0] )\n",
    "print( \"data size :\", sys.getsizeof(feature_vectors) / 1000000, \"[MB]\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "こちらはデータが非常に大きくなっていますが、これは 0 という成分を陽に保持しているためです。<br>\n",
    "この段階で memory error が生じる場合は一度 kernel を restart して DATA_NUM の数を減らして再実行してください。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.ラベルデータの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回扱うデータセットは全てに negative, positive というラベルが振られています。<br>\n",
    "ここではそのラベルを neagtive → 0, neagtive → 1 とすることで二値判別問題のセットアップを構築します。<br>\n",
    "先ほど作った説明変数となる特徴ベクトルはnegative sample 700文とpositive sample 700文を縦につなげて作ったものなので、0が700個と1が700個並んでいるベクトルを作成すれば必要なラベルを作れます。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = np.r_[np.tile(0, DATA_NUM), np.tile(1, DATA_NUM)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正しい位置で0と1の振替がなされているか確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 1 1\n"
     ]
    }
   ],
   "source": [
    "print( labels[0], labels[DATA_NUM-1], labels[DATA_NUM], labels[2*DATA_NUM-1]  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.学習用データとテスト用データの作成方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "論文の記述によれば、データを偏りがないように3分割に分け、 three fold cross validation でモデルを評価しています。<br>\n",
    "ここでは乱数を生成して、データを3等分することで同様の状況を再現することにします。<br>\n",
    "結果の再現性を担保するために乱数の seed も設定しておきます。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(7789)\n",
    "\n",
    "shuffle_order = np.random.choice( 2*DATA_NUM, 2*DATA_NUM, replace=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成した乱数の中身を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length : 1400\n",
      "first 10 elements : [1235 1232  910  162  343 1160  221  545 1112 1322]\n"
     ]
    }
   ],
   "source": [
    "print( \"length :\", len(shuffle_order) )\n",
    "print( \"first 10 elements :\", shuffle_order[0:10] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分割したデータセットに含まれるラベル=1の数を数えることでデータの偏りが生じていないかを確認します。<br>\n",
    "明らかに偏りが生じてしまった場合は乱数のseedを設定し直します。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one third of the length : 466\n",
      "# of '1' in 1st set : 227\n",
      "# of '1' in 2nd set : 233\n",
      "# of '1' in 3rd set : 240\n"
     ]
    }
   ],
   "source": [
    "one_third_size = int( 2*DATA_NUM / 3. )\n",
    "print( \"one third of the length :\", one_third_size )\n",
    "\n",
    "print( \"# of '1' in 1st set :\", np.sum( labels[ shuffle_order[:one_third_size] ]  ) )\n",
    "print( \"# of '1' in 2nd set :\", np.sum( labels[ shuffle_order[one_third_size:2*one_third_size] ]  ) )\n",
    "print( \"# of '1' in 3rd set :\", np.sum( labels[ shuffle_order[2*one_third_size:] ]  ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.モデルを学習して精度を検証"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習に必要な関数を定義します。<br>\n",
    "ここではモデルとして{Support Vector Machine(SVM), Naive Bayes(NB), Random Forest(RF)}を用います。<br>\n",
    "モデルの性能測定は予測と答えが一致する数をカウントして正答率を求めることで実施します。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与えられたリストをN分割する関数を定義します。<br>\n",
    "割り切れない場合はうしろのリストに格納します。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def N_splitter(seq, N):\n",
    "    avg = len(seq) / float(N)\n",
    "    out = []\n",
    "    last = 0.0\n",
    "    \n",
    "    while last < len(seq):\n",
    "        out.append( seq[int(last):int(last + avg)] )\n",
    "        last += avg\n",
    "        \n",
    "    return np.array(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "望ましい動作をするか確認してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([range(0, 4), range(4, 9), range(9, 14)], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_splitter(range(14), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルの学習や予測のための関数を定義します。<br>\n",
    "- train_model : 説明変数とラベルと手法を与えることでモデルを学習する\n",
    "- predict : モデルと説明変数を与えることでラベルを予測する\n",
    "- evaluate_model : 予測したラベルと実際の答えの合致数を調べる\n",
    "- cross_validate : cross_validationを実行する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(features, labels, method='SVM', parameters=None):\n",
    "    ### set the model\n",
    "    if method == 'SVM':\n",
    "        model = svm.SVC()\n",
    "    elif method == 'NB':\n",
    "        model = naive_bayes.GaussianNB()\n",
    "    elif method == 'RF':\n",
    "        model = RandomForestClassifier()\n",
    "    else:\n",
    "        print(\"Set method as SVM (for Support vector machine), NB (for Naive Bayes) or RF (Random Forest)\")\n",
    "    ### set parameters if exists\n",
    "    if parameters:\n",
    "        model.set_params(**parameters)\n",
    "    ### train the model\n",
    "    model.fit( features, labels )\n",
    "    ### return the trained model\n",
    "    return model\n",
    "\n",
    "def predict(model, features):\n",
    "    predictions = model.predict( features )\n",
    "    return predictions\n",
    "\n",
    "def evaluate_model(predictions, labels):\n",
    "    data_num = len(labels)\n",
    "    correct_num = np.sum( predictions == labels )\n",
    "    return data_num, correct_num\n",
    "\n",
    "def cross_validate(n_folds, feature_vectors, labels, shuffle_order, method='SVM', parameters=None):\n",
    "    result_test_num = []\n",
    "    result_correct_num = []\n",
    "    \n",
    "    n_splits = N_splitter( range(2*DATA_NUM), n_folds )\n",
    "\n",
    "    for i in range(n_folds):\n",
    "        print( \"Executing {0}th set...\".format(i+1) )\n",
    "        \n",
    "        test_elems = shuffle_order[ n_splits[i] ]\n",
    "        train_elems = np.array([])\n",
    "        train_set = n_splits[ np.arange(n_folds) !=i ]\n",
    "        for j in train_set:\n",
    "            train_elems = np.r_[ train_elems, shuffle_order[j] ]\n",
    "        train_elems = train_elems.astype(np.integer)\n",
    "\n",
    "        # train\n",
    "        model = train_model( feature_vectors[train_elems], labels[train_elems], method, parameters )\n",
    "        # predict\n",
    "        predictions = predict( model, feature_vectors[test_elems] )\n",
    "        # evaluate\n",
    "        test_num, correct_num = evaluate_model( predictions, labels[test_elems] )\n",
    "        result_test_num.append( test_num )\n",
    "        result_correct_num.append( correct_num )\n",
    "    \n",
    "    return result_test_num, result_correct_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記関数はCross validationの数を変数として設定できるように作ってあります。<br>\n",
    "今回は上述のように 3-folds で分析を行います。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_FOLDS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "準備ができたのでここでモデルを学習してその精度を確認してみましょう。<br>\n",
    "とりあえず何も考えずに準備した Bag Of Words をインプットにしてモデルを学習し、その精度を確認してみます。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing 1th set...\n",
      "Executing 2th set...\n",
      "Executing 3th set...\n",
      "CPU times: user 11.3 s, sys: 76.5 ms, total: 11.4 s\n",
      "Wall time: 11.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ans,corr = cross_validate(N_FOLDS, feature_vectors_csr, labels, shuffle_order, method='SVM', parameters=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結果を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average precision :  62.9 %\n"
     ]
    }
   ],
   "source": [
    "print( \"average precision : \", np.around( 100.*sum(corr)/sum(ans), decimals=1 ), \"%\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後の一文などで確かに positive な評価をしているレビューだということが見て取れます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.パラメタチューニング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grid searchでパラメタをチューニングすることで、どの程度精度が上がるかを確認してみます。<br>\n",
    "scikit-learnにはgrid searchが実装されているので、ここではそれを用いてパラメタをチューニングしてみます。<br>\n",
    "grid search に関しては http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html など。<br>\n",
    "下記セルの実行には4,5分程度かかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 53s, sys: 1.01 s, total: 3min 54s\n",
      "Wall time: 3min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "search_parameters = [\n",
    "    {'kernel': ['rbf'], 'gamma': [1e-2, 1e-3, 1e-4], 'C': [0.1, 1, 10, 100, 1000]},\n",
    "    {'kernel': ['linear'], 'C': [0.1, 1, 10, 100, 1000]}\n",
    "]\n",
    "\n",
    "model = svm.SVC()\n",
    "clf = grid_search.GridSearchCV(model, search_parameters)\n",
    "clf.fit( feature_vectors_csr, labels )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grid searchによって発見したパラメタやスコアを確認してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best paremters :  {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "best scores :  0.7935714285714286\n"
     ]
    }
   ],
   "source": [
    "print(\"best paremters : \", clf.best_params_)\n",
    "print(\"best scores : \", clf.best_score_)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "ans,corr = cross_validate(N_FOLDS, feature_vectors_csr, labels, shuffle_order, method='SVM', parameters=clf.best_params_)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "nbpresent": {
   "slides": {},
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
