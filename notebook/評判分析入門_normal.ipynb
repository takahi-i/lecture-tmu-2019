{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b0d37628-d11d-43ca-bd70-44567aebbf41"
    }
   },
   "source": [
    "# 評判分析で文章のポジネガを判別しよう\n",
    "\n",
    "機械学習を用いた評判分析における記念碑的論文( http://www.cs.cornell.edu/home/llee/papers/sentiment.pdf )と同様のセットアップで分析を行い、論文の精度を上回れるかチャレンジしてみましょう！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b14e9b2d-e81d-4c46-aaa5-a21ead865efb"
    }
   },
   "source": [
    "## 0.前準備\n",
    "\n",
    "python versionの確認します。<br>\n",
    "jupyter notebookではシェルコマンドの文頭に\"!\"をつけるとそのシェルコマンドをnotebook上で実行することができます。<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.5.2 :: Anaconda 4.1.1 (x86_64)\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "カレントディレクトリの確認とデータディレクトリの確認をします。<br>\n",
    "osモジュールを使うことでOS依存の機能を使えるようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'data', 'README.md', '評判分析入門_advanced.ipynb', '評判分析入門_normal.ipynb']\n"
     ]
    }
   ],
   "source": [
    "print( os.listdir(os.path.normpath(\"./\")) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mix20_rand700_tokens.zip', 'README', 'tokens']\n"
     ]
    }
   ],
   "source": [
    "print( os.listdir(os.path.normpath(\"./data/\")) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.dataの読み込みとモジュールのインポート"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyenvなどを用いているとpandasなどがimportできない場合があります。<br>\n",
    "その可能性の１つとしてlocale（国毎に異なる単位）の設定不足があり得るので、ここではそれを明示的に操作します。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your default locale is None\n",
      "Your locale is set as ja_JP.UTF-8\n"
     ]
    }
   ],
   "source": [
    "def set_locale():\n",
    "    default = os.environ.get('LC_ALL')\n",
    "    print( \"Your default locale is\", default )\n",
    "    if default is None:\n",
    "        os.environ.setdefault('LC_ALL', 'ja_JP.UTF-8')\n",
    "        print( \"Your locale is set as ja_JP.UTF-8\" )\n",
    "\n",
    "set_locale()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回使うデータファイルのパスをpythonのリストとして取得します。<br>\n",
    "globはパス名を見つけたりparseしたりするモジュールです( http://docs.python.jp/3/library/glob.html )。<br>\n",
    "今回扱うデータは https://www.cs.cornell.edu/people/pabo/movie-review-data/ より取得しています。<br>\n",
    "データ構造は下記のようになっています。<br>\n",
    "- data\n",
    "    - README\n",
    "    - tokens\n",
    "        - neg\n",
    "            - file1.txt\n",
    "            - file2.txt\n",
    "            - ...\n",
    "        - pos\n",
    "            - file1.txt\n",
    "            - file2.txt\n",
    "            - ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "neg_files = glob.glob( os.path.normpath(\"./data/tokens/neg/*\") )\n",
    "pos_files = glob.glob( os.path.normpath(\"./data/tokens/pos/*\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "取得したファイルパスの確認。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/tokens/neg/cv000_tok-9611.txt', 'data/tokens/neg/cv001_tok-19324.txt']\n",
      "['data/tokens/pos/cv000_tok-11609.txt', 'data/tokens/pos/cv001_tok-10180.txt']\n"
     ]
    }
   ],
   "source": [
    "print(neg_files[0:2])\n",
    "print(pos_files[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データ読み込みのテストをします。\n",
    "\n",
    "実際に文章を１つ読み込んでみて正しく読み込めているかを確認します。<br>\n",
    "本データは１つのファイルに１文で映画のレビュー文章が記載されています。<br>\n",
    "テキストの読み込みはエンコーディングの問題などでエラーが生じやすいので、慣れるまでは根気強くdebugしましょう。<br>\n",
    "\n",
    "無事に読み込めたら、具体的に１つファイルの中身を読み込んで内容を確認してみましょう。<br>\n",
    "sys はファイルサイズ取得などのシステム上の操作を行うモジュールです( http://docs.python.jp/3/library/sys.html )。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def text_reader(file_path):\n",
    "    python_version = sys.version_info.major\n",
    "    \n",
    "    if python_version >= 3:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                print(line)\n",
    "    else:\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* ( out of 4 = poor ) 1995 , g , 90 minutes [1 hour , 30 minutes] [comedy] starring : michael roescher ( hank royce ) , kristy young ( jinnie sue macallister ) , justin garms ( voice of gordy ) , james donadio ( gilbert sipes ) , written by leslie stevens , jay sommers , dick chevillat , produced by sybil robson , directed by mark lewis . \" gordy \" is not a movie , it is a 90-minute-long \" sesame street \" skit , and a very bad one at that . this movie is so stupid and dumb that it's depressing to think that some hollywood executives actually gave this the green light , and even more surprising is the fact that this is a disney movie . i'm sure children are the target audience of this movie , but only kids under the age of five may be able to tolerate it . it is the story of a farm a piglet named gordy ( voiced by garms ) , whose family has been taken away to \" up north , \" which we know means death . of course we can hear the animals talk to each other , and they actually went to the trouble of attempting to sync the voices with their mouths but it comes out terrible . actually , it's almost funny in a way . the only remotely interesting and likable character soon appears , a little girl named jinnie sue macallister ( young ) who sees gordy on the back of a truck and essentially steals him . jinnie is a country singer and the film goes off on a huge tangent to show her little concert and the people dancing to it . what is the point of this ? maybe she is one of the producer's relatives and they wanted to show her on camera to promote her or something . we then cut to a huge social gathering and drop in on another young kid named hank royce ( roescher ) who is sad because his divorced mother is dating . he leaves the party and meets jinnie sue , but he accidentally falls in a pool ( probably because he was sitting on the diving board with a $200 suit on - nah , didn't see that one coming ! ) , starts to drown , and is miraculously saved as gordy pushes an inflatable float over to him and saves him . if this had not been insanely stupid already the story quickly changes when jinnie gives gordy to hank who then ends up becoming the ceo of a food processing corporation when hank's grandfather , the original ceo , dies and leaves his fortune to hank . . . and gordy ! of course there must be a villain , but even this villain ( donadio as sipes ) isn't that evil . he never raises his voice or becomes angry , and of course he has the typical idiot goons kidnap gordy but this is just so beyond stupid and cartoony we are constantly two steps ahead of the story . it's hard to tell whether the overall corniness and cheesiness to the movie is intentional because it is a family film , or if the filmmaker's are just this untalented and stupid . at times \" gordy \" is tolerable to watch , thus earning it one star and not the dreaded \" z- . \" but it's just so unbelievably boring , cliche , dumb , unfunny , corny , and just plain bad it may scare children , it certainly disturbed me . ( 4/21/96 ) ( 1/29/97 ) ( 6/13/97 ) [see also : \" babe \" ]please visit chad'z movie page @ <a href= \" http : //members . aol . com/chadpolenz/index . html \" >http : //members . aol . com/chadpolenz/index . html</a> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_reader(neg_files[11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回使うモジュールの情報をまとめておきます。<br>\n",
    "詳細な中身などに関してはご自身で調べてみてください。<br>\n",
    "- matplotlib : グラフなどを描写する<br>\n",
    "http://matplotlib.org/\n",
    "- pandas : dataframeでデータを扱い、集計や統計量算出などがすぐ実行できる<br>\n",
    "http://pandas.pydata.org/\n",
    "- collections : pythonで扱えるデータの型を提供する<br>\n",
    "http://docs.python.jp/3/library/collections.html\n",
    "- numpy : 行列などの数学的オブジェクトを扱う<br>\n",
    "http://www.numpy.org/\n",
    "- sklearn.feature_extraction.DictVectorizer : 辞書データをベクトルの形に変換する<br>\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html\n",
    "- sklearnのモデル : SVM, NB, RF<br>\n",
    "http://scikit-learn.org/stable/tutorial/basic/tutorial.html\n",
    "- sklearn.grid_search : パラメタの最適な組み合わせ見つける<br>\n",
    "http://scikit-learn.org/stable/modules/grid_search.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib # not used in this notebook\n",
    "import pandas as pd # not used in this notebook\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "from sklearn import svm, naive_bayes\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn import grid_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.特徴ベクトルの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unigramを作成する関数を定義します。<br>\n",
    "スペース区切りで単語を抽出し、その数をカウントする単純な関数となります。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def word_counter(string):\n",
    "    words = string.strip().split()\n",
    "    count_dict = collections.Counter(words)\n",
    "    return dict(count_dict)\n",
    "\n",
    "def get_unigram(file_path):\n",
    "    result = []\n",
    "    python_version = sys.version_info.major\n",
    "    \n",
    "    if python_version >= 3:\n",
    "        for file in file_path:\n",
    "            with open(file, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    count_dict = word_counter(line)\n",
    "                    result.append(count_dict)\n",
    "    else:\n",
    "        for file in file_path:\n",
    "            with open(file, 'r') as f:\n",
    "                for line in f:\n",
    "                    count_dict = word_counter(line)\n",
    "                    result.append(count_dict)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "関数の挙動を確認してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I': 2,\n",
       " 'YK.': 1,\n",
       " 'am': 1,\n",
       " 'analysis': 1,\n",
       " 'data': 1,\n",
       " 'love': 1,\n",
       " 'python.': 1,\n",
       " 'using': 1}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counter(\"I am YK. I love data analysis using python.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この関数を用いて、negative と positive 両方で unigram を作成します。<br>\n",
    "得られた2つのリストを合わせてモデルのインプット（説明変数）とします。\n",
    "リストの結合は \"+\" で実施できます。 ex.) [1] + [2] = [1,2]<br>\n",
    "negative と positive は各700文ずつありますが、そのうちいくつを使うかをここで指定します。初期設定では全てのデータを使うことになっていますが、後の過程で memory 不足になるようでしたらこの数を減らしてください。<br>\n",
    "\n",
    "また、 jupyter notebook では %% をつけることで magic commands ( https://ipython.org/ipython-doc/3/interactive/magics.html ) という便利なコマンドを実行できます。ここでは処理にかかる時間をセルに表示するコマンドを使用しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 390 ms, sys: 120 ms, total: 510 ms\n",
      "Wall time: 836 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "DATA_NUM = 700\n",
    "\n",
    "unigrams_data = get_unigram(neg_files[:DATA_NUM]) + get_unigram(pos_files[:DATA_NUM])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得られたunigram_dataを確認してみます。単語の出現数がカウントされていることが確認できます。<br>\n",
    "合わせてそのデータサイズも確認してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'park': 1, 'yaz': 3, 'each': 1, 'boy': 1, 'and': 17, 'jack': 1, 'out': 3, ')': 6, 'climax': 1, 'counter-terrorist': 1, 'he': 4, 'soldiers': 1, 'extremely': 1, 'guilty': 1, 'feels': 1, \"it's\": 2, 'e-mail': 1, 'vibrant': 1, 'leaves': 1, 'segment': 1, 'rub': 1, 'hollywood': 1, 'href=': 2, 'mass': 1, 'result': 2, 'requires': 1, 'than': 1, 'gangster': 1, 'up': 4, 'else': 1, 'quinn': 5, 'is': 9, 'superstar': 1, 'lite': 1, 'well': 1, 'wife': 1, 'language': 1, 'rourke': 2, 'genre': 1, 'as': 1, 'ex-cia': 1, 'camera': 1, 'tsui': 2, 'much': 4, 'finds': 1, 'an': 5, 'looks': 1, 'jean-claude': 2, 'shirtless': 1, '>http': 1, 'motorcycle': 1, '\"': 22, 'amusement': 1, '.': 31, 'mention': 1, 'opponent': 1, '//www': 2, '>jpeck1@gl': 1, 'wacky': 1, 'mones': 1, 'fits': 1, 'persona': 1, 'us': 2, '30': 1, 'role': 2, 'work': 2, 'them': 2, 'van': 4, 'rodman': 7, \"stavros'\": 1, 'mercenary': 1, 'explosions': 1, 'timecop': 1, 'peck': 1, 'just': 3, 'job': 1, 'but': 4, 'monks': 1, 'to': 20, 'blatantly': 1, 'they': 1, 'rather': 1, 'in': 8, 'plays': 1, '(': 6, 'on': 5, 'operative': 1, 'of': 3, 'think': 1, 'therefore': 1, 'screenplay': 2, 'lunch': 1, 'visit': 1, 'gunfire': 1, 'teams': 1, 'known': 1, 'gratuitous': 1, 'paul': 2, 'for': 4, 'down': 2, 'mailto': 1, 'botched': 1, 'deal': 1, 'the': 20, 'always': 1, 'avoid': 1, 'kill': 2, 'tipsy': 1, 'steven': 1, 'home': 1, 'r': 1, 'stars': 1, 'pacing': 1, '1': 1, 'seagal': 1, 'actually': 1, 'we': 2, 'entertaining': 1, 'namely': 1, 'high': 1, 'though': 1, 'rome': 1, 'defend': 1, 'yet': 1, 'head-to-head': 1, 'legs': 1, 'coliseum': 1, 'screen': 1, 'jakoby': 1, 'need': 2, 'circles': 1, '/': 3, 'headache-inducing': 1, 'over': 1, 'figure': 1, 'mines': 1, 'since': 1, 'dennis': 3, 'rescue': 1, 'devoted': 1, 'frenetic': 1, 'hold': 1, 'http': 1, \"didn't\": 1, 'so': 3, 'hairdos': 1, 'land': 1, 'freeman': 1, \"neither's\": 1, 'tries': 2, 'aim': 1, 'exhausted': 1, \"1994's\": 1, 'juices': 1, 'baby': 1, 'tell': 1, 'running': 1, 'gets': 1, 'brite': 1, 'things': 1, 'edu/~jpeck1/': 1, 'performance': 1, 'must': 1, 'even': 1, 'notch': 1, 'set': 1, 'two': 1, '1997': 2, 'action': 1, 'be': 2, 'umbc': 4, 'that': 10, 'edu/~jpeck1/</a>': 1, '--': 3, 'double': 5, 'pretty': 2, \"he's\": 3, 't': 1, 'gl': 2, 'rarely': 1, 'possesses': 1, 'can': 1, ':': 9, 'happen': 1, 'a': 13, 'made': 1, 'beefy': 1, 'where': 1, 'profanity': 1, 'world': 1, 'revenge': 1, 'no': 1, 'who': 1, 'too': 3, 'online': 1, 'stavros': 4, 'arms': 1, 'by': 1, 'you': 1, 'woman': 1, 'tank': 1, 'death': 1, 'should': 2, 'career': 1, 'ripe': 1, 'witty': 1, 'dangerous': 1, 'been': 1, 'some': 2, 'could': 1, 'all': 4, 'this': 2, 'one': 2, 'watches': 1, 'it': 2, 'whatever': 1, 'peter': 1, 'power': 1, 'movie': 3, 'son': 1, 'dan': 1, \"there's\": 1, 'locks': 1, 'deadly': 1, 'strange': 1, 'often': 1, 'weird-looking': 1, 'whole': 1, 'albeit': 1, 'island': 1, 'pregnant': 1, 'mildly': 1, 'eye-popping': 1, 'indulge': 1, \"quinn's\": 1, 'story': 1, '?': 3, 'nba': 1, 'mickey': 2, 'needs': 2, 'movie-bedpost': 1, \"hark's\": 1, 'numerous': 1, 'have': 2, 'pleasure': 1, 'her': 1, \"rodman's\": 1, 'expert': 1, 'trying': 1, 'care': 1, 'bomb': 1, 'weird': 1, 'decided': 1, 'when': 2, 'escapes': 1, \"who's\": 1, 'enjoyable': 1, 'off': 1, 'leading': 1, 'bad': 2, 'killed': 1, 'do': 4, 'colorful': 1, 'natacha': 2, 'really': 1, 'hark': 1, 'his': 4, 'stunt': 1, 'jamie': 1, 'formula': 1, 'kicks': 1, 'lindinger': 2, 'frame': 1, 'exuberantly': 1, 'never': 3, 'going': 3, 'weapons': 1, ';': 6, '<a': 2, 'save': 1, 'cast': 1, 'did': 2, 'equipped': 1, 'kickboxing': 1, 'from': 1, 'major': 1, 'kidnaps': 1, 'damme': 4, 'net-surfing': 1, 'if': 1, 'critic': 1, 'director': 1, 'acting': 1, 'bleachers': 1, \"i've\": 1, 'edu': 1, 'taken': 1, 'charisma': 1, 'between': 1, 'team': 5, 'tiger': 1, 'travel': 1, 'enough': 1, 'roman': 1, 'back': 1, 'are': 1, 'free': 1, 'another': 2, 'scenes': 1, 'colony': 1, 'dealer': 1, 'while': 2, 'about': 1, 'crazy': 1, 'loud': 1, 'reel': 1, 'valuable': 1, 'i': 2, 'with': 5, 'level': 1, 'basket': 1, 'pau': 1, 'make': 1, 'tristar': 1, 'comes': 1, \"what's\": 3, 'jpeck1@gl': 1, 'violence': 1, 'antwerp': 1, 'killathon': 1, 'exhilarated': 1, ',': 25, 'slows': 1, 'edu</a>': 1}\n",
      "data size : 0.011264 [MB]\n"
     ]
    }
   ],
   "source": [
    "print( unigrams_data[0] )\n",
    "print( \"data size :\", sys.getsizeof(unigrams_data) / 1000000, \"[MB]\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上で得られたデータは unigram という 1400 の要素を持つリストであり、各要素は key と value からなる辞書となっています。<br>\n",
    "これを扱いやすい行列の形にします。<br>\n",
    "ここでは各行が１つのレビューテキストに対応するようにして、各列が単語、要素がその単語の出現数というデータを作成します。<br>\n",
    "scikit-learn で実装されている DictVectorizer という関数を使うことでそれが簡単に実行できます。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 724 ms, sys: 33.7 ms, total: 758 ms\n",
      "Wall time: 774 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vec = DictVectorizer()\n",
    "feature_vectors_csr = vec.fit_transform( unigrams_data )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作成したデータを確認してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1400x44219 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 496525 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_vectors_csr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "行列の全成分（行成分×列成分）は 60,000,000 要素くらいありますが、このうちのほとんどは 0 で 0 以外の値が入っているのは500,000程度です。<br>\n",
    "この CSR(Compressed Sparse Row) matrix というのはこのような疎行列をの 0 でない成分だけを保持する賢いものになっています。<br>\n",
    "\n",
    "一方で 0 の成分を陽に保って普通の行列としてデータを保持することも可能です。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data dimension : (1400, 44219)\n",
      "[  0.   0.  22. ...,   0.   0.   0.]\n",
      "data size : 495.252912 [MB]\n"
     ]
    }
   ],
   "source": [
    "feature_vectors = vec.fit_transform( unigrams_data ).toarray()\n",
    "print( \"data dimension :\", feature_vectors.shape )\n",
    "print( feature_vectors[0] )\n",
    "print( \"data size :\", sys.getsizeof(feature_vectors) / 1000000, \"[MB]\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "こちらはデータが非常に大きくなっていますが、これは 0 という成分を陽に保持しているためです。<br>\n",
    "この段階で memory error が生じる場合は一度 kernel を restart して DATA_NUM の数を減らして再実行してください。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.ラベルデータの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回扱うデータセットは全てに negative, positive というラベルが振られています。<br>\n",
    "ここではそのラベルを neagtive → 0, neagtive → 1 とすることで二値判別問題のセットアップを構築します。<br>\n",
    "先ほど作った説明変数となる特徴ベクトルはnegative sample 700文とpositive sample 700文を縦につなげて作ったものなので、0が700個と1が700個並んでいるベクトルを作成すれば必要なラベルを作れます。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = np.r_[np.tile(0, DATA_NUM), np.tile(1, DATA_NUM)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正しい位置で0と1の振替がなされているか確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 1 1\n"
     ]
    }
   ],
   "source": [
    "print( labels[0], labels[DATA_NUM-1], labels[DATA_NUM], labels[2*DATA_NUM-1]  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.学習用データとテスト用データの作成方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "論文の記述によれば、データを偏りがないように3分割に分け、 three fold cross validation でモデルを評価しています。<br>\n",
    "ここでは乱数を生成して、データを3等分することで同様の状況を再現することにします。<br>\n",
    "結果の再現性を担保するために乱数の seed も設定しておきます。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(7789)\n",
    "\n",
    "shuffle_order = np.random.choice( 2*DATA_NUM, 2*DATA_NUM, replace=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成した乱数の中身を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length : 1400\n",
      "first 10 elements : [1235 1232  910  162  343 1160  221  545 1112 1322]\n"
     ]
    }
   ],
   "source": [
    "print( \"length :\", len(shuffle_order) )\n",
    "print( \"first 10 elements :\", shuffle_order[0:10] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分割したデータセットに含まれるラベル=1の数を数えることでデータの偏りが生じていないかを確認します。<br>\n",
    "明らかに偏りが生じてしまった場合は乱数のseedを設定し直します。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one third of the length : 466\n",
      "# of '1' in 1st set : 227\n",
      "# of '1' in 2nd set : 233\n",
      "# of '1' in 3rd set : 240\n"
     ]
    }
   ],
   "source": [
    "one_third_size = int( 2*DATA_NUM / 3. )\n",
    "print( \"one third of the length :\", one_third_size )\n",
    "\n",
    "print( \"# of '1' in 1st set :\", np.sum( labels[ shuffle_order[:one_third_size] ]  ) )\n",
    "print( \"# of '1' in 2nd set :\", np.sum( labels[ shuffle_order[one_third_size:2*one_third_size] ]  ) )\n",
    "print( \"# of '1' in 3rd set :\", np.sum( labels[ shuffle_order[2*one_third_size:] ]  ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.モデルを学習して精度を検証"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習に必要な関数を定義します。<br>\n",
    "ここではモデルとして{Support Vector Machine(SVM), Naive Bayes(NB), Random Forest(RF)}を用います。<br>\n",
    "モデルの性能測定は予測と答えが一致する数をカウントして正答率を求めることで実施します。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与えられたリストをN分割する関数を定義します。<br>\n",
    "割り切れない場合はうしろのリストに格納します。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def N_splitter(seq, N):\n",
    "    avg = len(seq) / float(N)\n",
    "    out = []\n",
    "    last = 0.0\n",
    "    \n",
    "    while last < len(seq):\n",
    "        out.append( seq[int(last):int(last + avg)] )\n",
    "        last += avg\n",
    "        \n",
    "    return np.array(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "望ましい動作をするか確認してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([range(0, 4), range(4, 9), range(9, 14)], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_splitter(range(14), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルの学習や予測のための関数を定義します。<br>\n",
    "- train_model : 説明変数とラベルと手法を与えることでモデルを学習する\n",
    "- predict : モデルと説明変数を与えることでラベルを予測する\n",
    "- evaluate_model : 予測したラベルと実際の答えの合致数を調べる\n",
    "- cross_validate : cross_validationを実行する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(features, labels, method='SVM', parameters=None):\n",
    "    ### set the model\n",
    "    if method == 'SVM':\n",
    "        model = svm.SVC()\n",
    "    elif method == 'NB':\n",
    "        model = naive_bayes.GaussianNB()\n",
    "    elif method == 'RF':\n",
    "        model = RandomForestClassifier()\n",
    "    else:\n",
    "        print(\"Set method as SVM (for Support vector machine), NB (for Naive Bayes) or RF (Random Forest)\")\n",
    "    ### set parameters if exists\n",
    "    if parameters:\n",
    "        model.set_params(**parameters)\n",
    "    ### train the model\n",
    "    model.fit( features, labels )\n",
    "    ### return the trained model\n",
    "    return model\n",
    "\n",
    "def predict(model, features):\n",
    "    predictions = model.predict( features )\n",
    "    return predictions\n",
    "\n",
    "def evaluate_model(predictions, labels):\n",
    "    data_num = len(labels)\n",
    "    correct_num = np.sum( predictions == labels )\n",
    "    return data_num, correct_num\n",
    "\n",
    "def cross_validate(n_folds, feature_vectors, labels, shuffle_order, method='SVM', parameters=None):\n",
    "    result_test_num = []\n",
    "    result_correct_num = []\n",
    "    \n",
    "    n_splits = N_splitter( range(2*DATA_NUM), n_folds )\n",
    "\n",
    "    for i in range(n_folds):\n",
    "        print( \"Executing {0}th set...\".format(i+1) )\n",
    "        \n",
    "        test_elems = shuffle_order[ n_splits[i] ]\n",
    "        train_elems = np.array([])\n",
    "        train_set = n_splits[ np.arange(n_folds) !=i ]\n",
    "        for j in train_set:\n",
    "            train_elems = np.r_[ train_elems, shuffle_order[j] ]\n",
    "        train_elems = train_elems.astype(np.integer)\n",
    "\n",
    "        # train\n",
    "        model = train_model( feature_vectors[train_elems], labels[train_elems], method, parameters )\n",
    "        # predict\n",
    "        predictions = predict( model, feature_vectors[test_elems] )\n",
    "        # evaluate\n",
    "        test_num, correct_num = evaluate_model( predictions, labels[test_elems] )\n",
    "        result_test_num.append( test_num )\n",
    "        result_correct_num.append( correct_num )\n",
    "    \n",
    "    return result_test_num, result_correct_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記関数はCross validationの数を変数として設定できるように作ってあります。<br>\n",
    "今回は上述のように 3-folds で分析を行います。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_FOLDS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "準備ができたのでここでモデルを学習してその精度を確認してみましょう。<br>\n",
    "とりあえず何も考えずに準備した Bag Of Words をインプットにしてモデルを学習し、その精度を確認してみます。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing 1th set...\n",
      "Executing 2th set...\n",
      "Executing 3th set...\n",
      "CPU times: user 13.8 s, sys: 239 ms, total: 14.1 s\n",
      "Wall time: 15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ans,corr = cross_validate(N_FOLDS, feature_vectors_csr, labels, shuffle_order, method='SVM', parameters=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結果を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average precision :  63.2 %\n"
     ]
    }
   ],
   "source": [
    "print( \"average precision : \", np.around( 100.*sum(corr)/sum(ans), decimals=1 ), \"%\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際にどのような文章では予測が正解してどのような文章では予測を外しているのかを見てみます。<br>\n",
    "モデルを学習して予測を行い、データを見てみます。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.43 s, sys: 62.1 ms, total: 3.49 s\n",
      "Wall time: 4.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "svm_model = train_model(\n",
    "    features=feature_vectors_csr[shuffle_order[0:950],:]\n",
    "    , labels=labels[shuffle_order[0:950]]\n",
    "    , method='SVM'\n",
    "    , parameters=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data :  [ 490 1276  794  892  504  463   48  289 1218 1137] correct label :  [0 1 1 1 0 0 0 0 1 1]\n",
      "predict label :  [1 1 0 0 0 1 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"data : \" ,shuffle_order[970:980], \"correct label : \", labels[shuffle_order[970:980]])\n",
    "print( \"predict label : \", predict(svm_model, feature_vectors_csr[shuffle_order[970:980], :]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "予測が間違っているものを見てみます。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* directed and co-produced by jodie foster . written by w . d . richter . cast : holly hunter , robert downey , jr . , anne bancroft , dylan mcdermott , charles durning , geraldine chaplin , steve guttenberg , claire danes , cynthia stevenson . distributed by paramount and polygram . the title \" home for the holidays \" suggests either a feel-good story , a family drama or a comedy , but guesses can be wrong . there was a made-for-tv movie with that title in 1972 which turned out to be a christmastime thriller . now , the current film's distributors have announced it as a comedy , but this description will do only if you stretch it . claudia ( holly hunter ) just lost her job as an art restorer at a museum . ( you wait a long time to find out that the museum is in chicago . ) with some trepidation , claudia leaves behind her sixteen-year old daughter ( we later learn in passing that hunter is unmarried ) and flies to another state to spend thanksgiving with her parents . ( you wait a long time to find out that it is maryland . if your eyes are peeled , you might catch the sign \" maryland lottery . \" ) the opening sequence tries for screwball comedy style , but it is only slightly weird and far from hilarious . soon the film goes into fast decline , in a structure that is a succession of sections , each with its title . \" flying \" is followed by \" mom and dad . \" mom is anne bancroft , nee anna maria luisa italiano , one of the very few catholics who can do a jewish mother turn , which she sort of does here . perhaps husband mel brooks is of help . dad ( charles durning ) is a retired something or other employee of an airline or of the baltimore airport . ( this is made clear only at the end . ) he is a kind of putterer now . pa and ma are eccentrics , sort of , but uninteresting as individuals and as a couple . if you remember bancroft as the famously seductive mrs . robinson in 1967 the graduate , you can now see her in bra and slip . she may be showing us how well kept she is at 64 , but her talent does not show in home for the holidays . the next chapter , \" company , \" centers mostly around claudia's brother tommy ( robert downey , jr . ) . tommy is gay , but you can't tell . you learn it only when you hear that \" he has broken with jack . \" on the other hand , tommy is relentlessly gay in the old sense , ebullient , bouncy and far , far too cute . you feel right away that downey is uncomfortable in , and unconvinced by his role . he arrives from boston with leo fish ( dylan mcdermott ) whom everybody takes for a new lover . each section has claudia in it but is normally focused on one or two additional characters . in \" relatives \" we meet mom's unmarried sister , retired schoolteacher geraldine chaplin . she too is an eccentric , unconvincing and unconvinced . her closeups are cruel . someone flatulates . there's an in-joke : a furnace repairman works for \" the big heat \" company , a reference to a classic film noir . in \" more relatives , \" we meet claudia's younger sister joanna , her sententious husband walter ( steve guttenberg ) and their two kids . the house becomes a zoo of agitation and hyperactivity , forced and seldom funny . only the cat is normal . next , \" the dinner \" reveals that tommy had married jack some time ago . joanna calls tommy a freak . he keeps his good mood but overacts again . by accident , joanna gets a plateful spilled on her dress . it turns out that leo fish is heterosexual . geraldine chaplin passionately kisses dad--on the mouth . watching the slicing of the turkey , it dawned on me that this film was trying to be a slice of life . but it is also a turkey that jodie foster directed--her second effort , after little man tate . just as responsible is the uneven writer w . d . richter , whose biggest hit was the dubious buckaroo banzai . his best works were slither , nickelodeon , and the excellent remake of invasion of the body snatchers . the stephen king story he scripted before home for the holidays was the disastrous needful things . in \" cleanup \" claudia's daughter calls , tells mom that she is disgusted with her boyfriend and is still a virgin . tommy and walter have a fight . jack telephones tommy . tommy is happy . the marriage is on again . the movie turns \" sensitive . \" \" now what ? \" follows . here , this mish-mashy , misguided , mis-written film predictably brings together leo fish and claudia . earlier , leo was passable as a quiet observer . now that he emotes , he is artificial . the couple have a too-cute impromptu date and kiss . taking leftovers to joanna's house , the sisters have a heart-to-heart talk . says claudia : \" we don't have to like each other . we're a family . \" this , i guess , is the moral of the story . viewer liberation comes with \" the point . \" claudia and leo , by mutual consent , will not have sex as they are going in different geographic ( and other ) directions . dad watches home movies . more sentiment . the real point is that this movie is pointless . that not a single performer is explored , seems to believe in his part , plays well , is involving , that no one uses more than two basic expressions . claudia takes the return plane . who comes in but leo , with a ridiculous lamp ( don't ask ) ? he proposes that they spend the two-hour flight sitting together . perhaps they'll go to sleep . that's what the film's audience ought to do too . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_reader(neg_files[490-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "長いので全部読むのは大変ですが、読んでみると確かに negative なレビューだとわかります。<br>\n",
    "一方で単語だけ見ると　feel-good や happy など positive と捉えられるような単語も出現しています。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "予測が当たっているものも見てみます。<br>\n",
    "ファイルの指定方法に注意してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in these days of overlong movies ( meet joe black , the thin red line , the mask of zorro ) it is a shame that films like waking ned devine can't be longer than a paltry 90 minutes . this is just a cute movie , even through its mildly risque subject matter . old friends jackie ( bannen ) and michael ( kelley ) try to find the lottery winner ( they deduce must live in their dinky town of about 60 ) so that they might kiss up to him and share the winnings . through process of elimination , they find that it must be lovable old ned devine , who they find sitting in front of his tv , clutching the winning lottery ticket in his cold dead hand . what results is thuroughly amusing , as jackie tries to convince his wife that not claiming it would be wrong , and that they could really benefit . after all , old ned won't miss it . rather than divulge the later twists and turns , i'll stop here merely pointing out that jackie and michael get into all sorts of trouble in their little sleepy irish villiage . bannen and kelly are a perfect pair . one slightly stout , the other as thin as a rail . both getting on in years , they make such a cute pair of old codgers . waking ned devine may even be seen as a \" full monty \" for the geriatric set , especially since kelly gets buff-o for one amusing scene . waking ned devine is by no means perfect , but it is so sincere and touching that it looks so much better than most films . the performances by everyone in the town are great , particularly the two leads . there is one twist at the end which i find unnecessary , but it hardly ruins the picture . writer/director kirk jones should be held up as an example to all those hollywood screenwriters . scripts as creative and endearing as this should be the norm , not the exception . perhaps it makes us appreciate this wonderful film even more . had i held off on my year's best/worst list for another day , waking ned devine ( officially released in late november of 98 ) surely would have graced the short group of the year's finest films . it is light , but thought provoking and sweet . i can't think of anyone who shouldn't see ( or wouldn't enjoy ) this film . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_reader(pos_files[1276-700-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後の一文などで確かに positive な評価をしているレビューだということが見て取れます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.パラメタチューニング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grid searchでパラメタをチューニングすることで、どの程度精度が上がるかを確認してみます。<br>\n",
    "scikit-learnにはgrid searchが実装されているので、ここではそれを用いてパラメタをチューニングしてみます。<br>\n",
    "grid search に関しては http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html など。<br>\n",
    "下記セルの実行には4,5分程度かかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 36s, sys: 2.85 s, total: 4min 39s\n",
      "Wall time: 4min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "search_parameters = [\n",
    "    {'kernel': ['rbf'], 'gamma': [1e-2, 1e-3, 1e-4], 'C': [0.1, 1, 10, 100, 1000]},\n",
    "    {'kernel': ['linear'], 'C': [0.1, 1, 10, 100, 1000]}\n",
    "]\n",
    "\n",
    "model = svm.SVC()\n",
    "clf = grid_search.GridSearchCV(model, search_parameters)\n",
    "clf.fit( feature_vectors_csr, labels )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grid searchによって発見したパラメタやスコアを確認してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best paremters :  {'gamma': 0.0001, 'C': 100, 'kernel': 'rbf'}\n",
      "best scores :  0.797857142857\n"
     ]
    }
   ],
   "source": [
    "print(\"best paremters : \", clf.best_params_)\n",
    "print(\"best scores : \", clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing 1th set...\n",
      "Executing 2th set...\n",
      "Executing 3th set...\n",
      "CPU times: user 13.3 s, sys: 113 ms, total: 13.5 s\n",
      "Wall time: 13.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ans,corr = cross_validate(N_FOLDS, feature_vectors_csr, labels, shuffle_order, method='SVM', parameters=clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average precision :  79.5 %\n"
     ]
    }
   ],
   "source": [
    "print( \"average precision : \", np.around( 100.*sum(corr)/sum(ans), decimals=1 ), \"%\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "精度が15%程度も向上しました！機械学習においてモデルのパラメタチューニングが非常に重要であることが伺えます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.簡単な特徴量変換による効果の確認"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "論文に記載してあるように、Bag of Words(BoW) のカウント数を全て1にしてみることで精度にどのような変化が生じるかを調べてみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_vectors_csr.data[ feature_vectors_csr.data > 0 ] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "変換したデータを用いて同様に学習プロセスを実行してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing 1th set...\n",
      "Executing 2th set...\n",
      "Executing 3th set...\n",
      "CPU times: user 14.4 s, sys: 118 ms, total: 14.6 s\n",
      "Wall time: 14.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ans, corr = cross_validate(N_FOLDS, feature_vectors_csr, labels, shuffle_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average precision :  49.1 %\n"
     ]
    }
   ],
   "source": [
    "print( \"average precision : \", np.around( 100.*sum(corr)/sum(ans), decimals=1 ), \"%\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "なんと精度がだいぶ落ちてしまいました。。。<br>\n",
    "しかも現在の問題設定は 0 か 1 を判別するものなので、ランダムに判別するモデルを作ったとしても 50% 程度になります。<br>\n",
    "それと同程度ということは、そもそもモデルの学習が上手くいっていないのではないかということが疑われます。<br>\n",
    "そのことを検証してみるために、もう一度パラメタチューニングを実施してみます。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 40s, sys: 2.05 s, total: 4min 42s\n",
      "Wall time: 4min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "search_parameters = [\n",
    "    {'kernel': ['rbf'], 'gamma': [1e-2, 1e-3, 1e-4], 'C': [0.1, 1, 10, 100, 1000]},\n",
    "    {'kernel': ['linear'], 'C': [0.1, 1, 10, 100, 1000]}\n",
    "]\n",
    "\n",
    "model = svm.SVC()\n",
    "clf = grid_search.GridSearchCV(model, search_parameters)\n",
    "clf.fit( feature_vectors_csr, labels )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best paremters :  {'gamma': 0.001, 'C': 10, 'kernel': 'rbf'}\n",
      "best scores :  0.814285714286\n"
     ]
    }
   ],
   "source": [
    "print(\"best paremters : \", clf.best_params_)\n",
    "print(\"best scores : \", clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing 1th set...\n",
      "Executing 2th set...\n",
      "Executing 3th set...\n",
      "CPU times: user 14.1 s, sys: 107 ms, total: 14.2 s\n",
      "Wall time: 14.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ans, corr = cross_validate(N_FOLDS, feature_vectors_csr, labels, shuffle_order, method='SVM', parameters=clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average precision :  82.7 %\n"
     ]
    }
   ],
   "source": [
    "print( \"average precision : \", np.around( 100.*sum(corr)/sum(ans), decimals=1 ), \"%\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "パラメタを調整することで高い精度を発揮することが分かりました！<br>\n",
    "この精度は論文に記載されているもの(82.9%)とほぼ同程度のものとなっています。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.SVM以外のモデルを実行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes は sparse matrix 型のインプットを受け付けないので、numpy arrayとして作成したデータを入れなければなりません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing 1th set...\n",
      "Executing 2th set...\n",
      "Executing 3th set...\n",
      "average precision :  62.3 %\n",
      "CPU times: user 3.61 s, sys: 3.41 s, total: 7.02 s\n",
      "Wall time: 7.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ans, corr = cross_validate(N_FOLDS, feature_vectors, labels, shuffle_order, method='NB')\n",
    "print( \"average precision : \", np.around( 100.*sum(corr)/sum(ans), decimals=1 ), \"%\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest も実行してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing 1th set...\n",
      "Executing 2th set...\n",
      "Executing 3th set...\n",
      "average precision :  64.9 %\n",
      "CPU times: user 2.37 s, sys: 841 ms, total: 3.22 s\n",
      "Wall time: 3.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ans, corr = cross_validate(N_FOLDS, feature_vectors, labels, shuffle_order, method='RF')\n",
    "print( \"average precision : \", np.around( 100.*sum(corr)/sum(ans), decimals=1 ), \"%\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gaussianNBはチューニング可能なパラメタはありませんが、RFはパラメタが多いため、興味がある方は下のセルのコメントアウトを外して、パラメタによって結果がどう変わるかを調べてみてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# search_parameters = {\n",
    "#     \"max_depth\": [3, 5, None],\n",
    "#     \"max_features\": [1000,5000,10000,20000,30000],\n",
    "#     \"min_samples_split\": [1,2,3,4,5,10],\n",
    "#     \"min_samples_leaf\": [5,10,20,50],\n",
    "#     \"bootstrap\": [True, False],\n",
    "#     \"criterion\": [\"gini\", \"entropy\"]\n",
    "# }\n",
    "\n",
    "# model = RandomForestClassifier()\n",
    "# clf = grid_search.GridSearchCV(model, search_parameters)\n",
    "# clf.fit( feature_vectors_csr, labels )\n",
    "\n",
    "# print(clf.best_params_)\n",
    "\n",
    "# ans, corr = cross_validate(N_FOLDS, feature_vectors_csr, labels, shuffle_order, method='RF', parameters=clf.best_params_)\n",
    "# print( \"average precision : \", np.around( 100*sum(corr)/sum(ans), decimals=1 ), \"%\" )"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nbpresent": {
   "slides": {},
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
